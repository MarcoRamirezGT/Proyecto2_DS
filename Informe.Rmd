---
title: "Proyecto 2"
author: "Marco Ramirez, Estuardo Hernandez, Alfredo Quezada"
date: "2022-09-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Prediccion de argumentos efectivos

El objetivo de este proyecto es clasificar los elementos argumentativos en la escritura de los estudiantes como "efectivos", "adecuados" o "ineficaces". Esto mediante los datos proporcionados por Kaggle.


### Situacion problematica

Partimos de la base, que una escrituta argumentativa ayuda a el crecimiento del pensamiento critito, dicho eso, solo el 13% de los catedraticos piden a sus alumnos 
que escriban de manera persuasiva, si quitamos ese 13% de alumnos que llegan a tener esa practica constante, resulta una cantidad de jovenes que no logran fomentar 
sus habilidades, si bien es cierto que a dia de hoy existe una cantidad abundante de herramientas en linea que permiten una retroalimentacion automatizada, no llega 
a similar ni siquiera igualar, la practica que se adquiere mediante el metodo tradicional. 



### Problema cientifico


Por tanto, nos topamos con el problema de que no todos desarrollan una calidad aceptable de elementos argumentativos, desarrollo de ideas o la organizacion de ideas, por lo tanto, se estable una investigacion para determinar la calidad de los argumentos, en base a su efectividad. 

### Objetivos

**- Objetivos generales: ** 

1. Catalogar los argumentos en base a su efectividad.  


2. Determinar el tipo de discurso con mayor efectividad. 


**- Obetivos especificos: **

1. Catalogar discurosos como **Ineficaz** , **Adeacuado** o **Eficaz**


2. Identificar los sentimientos obtenidos en base a los discursos realizados. 


### Descripcion de los datos

```{r Analisis Exploratorio}

db<- read.csv('train.csv')


```

El conjunto de datos contiene ensayos argumentativos escritos por estudiantes estadounidenses en los grados 6-12. Estos ensayos fueron anotados por calificadores expertos para los elementos del discurso que se encuentran comúnmente en la escritura argumentativa:

-Lead: una introducción que comienza con una estadística, una cita, una descripción o algún otro dispositivo para captar la atención del lector y apuntar hacia la tesis.

-Position: una opinión o conclusión sobre la pregunta principal.

-Claim: un reclamo que respalda la posición.

-Counterclaim: una afirmación que refuta otra afirmación o da una razón opuesta a la posición.

-Rebuttal: una afirmación que refuta una contrademanda.

-Evidence: ideas o ejemplos que respaldan afirmaciones, reconvenciones o refutaciones.

-Concluding Statement: una declaración de conclusión que reafirma las afirmaciones.

#### Descripción del dataframe

-discourse_id: código de identificación para el elemento de discurso. Variable categórica.
-essay_id: código de identificación para la respuesta del ensayo. Variable categórica.
-discourse_text: texto del elemento del discurso. Variable categórica.
-discourse_type: etiqueta de clase del elemento de discurso. Variable categórica.
-discourse_effectiveness: calificación de la calidad del elemento del discurso, el objetivo. Variable categórica.

#### Representación gráfica de los datos

```{r}
plot(x = db$discourse_id)
plot(x = db$essay_id)
```

### Analisis Exploratorio

La base de datos cuenta con `r nrow(db)` filas y `r ncol(db)` columnas


```{r message=FALSE, warning=FALSE}
# Librerias necesarias

library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("syuzhet")
library("ggplot2")
library(dplyr)
# Limpiamos el texto




```
#### 1. ¿Cuál es el tipo de discurso predominante?

```{r}

disType <- db %>% 
  group_by(db$discourse_type) %>% 
  tally()

colnames(disType)[1]<-'Tipo'
colnames(disType)[2]<-'Cantidad'

ggplot(data=disType, aes(x=reorder(Tipo,Cantidad), y=Cantidad,fill=Tipo)) +
  geom_bar(stat="identity", position=position_dodge())+
  geom_text(aes(label=Cantidad), vjust=1.6, color="black",
            position = position_dodge(0.9), size=3.5)+
  labs(title="Cantidad de cada tipo de discurso",x='Tipo de discurso', y="Cantidad")+
  theme(legend.position="none")
```



Como se observa en la grafica anterior el tipo de discurso predominante es 'Evidence', el cual se refiere a ideas o ejemplos que respaldan afirmaciones, reconvenciones o refutaciones. Y en en segundo puesto se encuentra 'Claim', el cual es un reclamo que respalda alguna posicion. 


#### Cantidad de discursos categorizados como ineficaz, adecuado y eficaz

```{r}
cantType <- db %>% 
  group_by(db$discourse_effectiveness) %>% 
  tally()

colnames(cantType)[1]<-'Tipo'
colnames(cantType)[2]<-'Cantidad'

ggplot(data=cantType, aes(x=reorder(Tipo,Cantidad), y=Cantidad,fill=Tipo)) +
  geom_bar(stat="identity", position=position_dodge())+
  geom_text(aes(label=Cantidad), vjust=1.6, color="black",
            position = position_dodge(0.9), size=3.5)+
  labs(title="Cantidad de cada efectividad de los discursos",x='Efectividad', y="Cantidad")+
  theme(legend.position="none")

```
<br />
Como se observa, los 3 tipos de efectividad no se encuentran balanceados, ademas, la efectividad predominante es 'Adequate' con una cantidad de '20977' discursos. 'Effective' con una cantidad de '9326' e 'Ineffective' con una cantidad de '6462'

#### Analisis del los discursos

##### Analisis del discurso sin limpieza
```{R warning=FALSE}

wordcloud(words = db$discourse_text, 
          max.words=100, random.order=FALSE, rot.per=0.40, 
          colors=brewer.pal(8, "Dark2"))



```
< br />
Como se observa en la mayoria de discursos, la palabra con mayor frecuencia es estudiantes, luego people, y electoral, entre ellas.

Ahora veremos como se ve una nube de palabras con una limpieza de datos.

```{r warning=FALSE}

TextDoc <- Corpus(VectorSource(db$discourse_text))
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
TextDoc <- tm_map(TextDoc, toSpace, "/")
TextDoc <- tm_map(TextDoc, toSpace, "@")
TextDoc <- tm_map(TextDoc, toSpace, "\\|")
# Convert the text to lower case
TextDoc <- tm_map(TextDoc, content_transformer(tolower))
# Remove numbers
TextDoc <- tm_map(TextDoc, removeNumbers)
# Remove english common stopwords
TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
# Remove your own stop word
# specify your custom stopwords as a character vector
TextDoc <- tm_map(TextDoc, removeWords, c("like", "just", "get",'will','new','now','via','dont','one','can')) 
# Remove punctuations
TextDoc <- tm_map(TextDoc, removePunctuation)
# Eliminate extra white spaces
TextDoc <- tm_map(TextDoc, stripWhitespace)
# Text stemming - which reduces words to their root form
TextDoc <- tm_map(TextDoc, stemDocument)

TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
 # Sort by descearing value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 5 most frequent words
head(dtm_d)
# Plot the most frequent words
barplot(dtm_d[1:5,]$freq, las = 2, names.arg = dtm_d[1:5,]$word,
        col ="lightgreen", main ="Top 5 most frequent words",
        ylab = "Word frequencies")
set.seed(1234)
wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 5,
          max.words=100, random.order=FALSE, rot.per=0.40, 
          colors=brewer.pal(8, "Dark2"))
```

<br /> 
Como se observa la palabra *Student* sigue predominando en los discursos de los estudiantes, asi como *people* y *vote*

##### Analisis de sentimiento en los discursos

```{r warning=FALSE}

syuzhet_vector <- get_sentiment(db$discourse_text, method="syuzhet")
# see the first row of the vector
head(syuzhet_vector)
# see summary statistics of the vector
summary(syuzhet_vector)

# bing
bing_vector <- get_sentiment(db$discourse_text, method="bing")
head(bing_vector)
summary(bing_vector)
#affin
afinn_vector <- get_sentiment(db$discourse_text, method="afinn")
head(afinn_vector)
summary(afinn_vector)

#compare the first row of each vector using sign function
rbind(
  sign(head(syuzhet_vector)),
  sign(head(bing_vector)),
  sign(head(afinn_vector))
)

d<-get_nrc_sentiment(db$discourse_text)
# head(d,10) - to see top 10 lines of the get_nrc_sentiment dataframe
head (d,10)
db$Enojo<- d$anger
db$Anticipacion<- d$anticipation
db$Disgusto<- d$disgust
db$Miedo<- d$fear
db$Felicidad<- d$joy
db$Tristeza<- d$sadness
db$Sorpresa<- d$surprise
db$Verdad<- d$trust
db$Negativo<- d$negative
db$Positivo<- d$positive



#transpose
td<-data.frame(t(d))
#The function rowSums computes column sums across rows for each level of a grouping variable.
td_new <- data.frame(rowSums(td[2:253]))
#Transformation and cleaning
names(td_new)[1] <- "count"
td_new <- cbind("sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2<-td_new[1:8,]
#Plot One - count of words associated with each sentiment
quickplot(sentiment, data=td_new2, weight=count, geom="bar", fill=sentiment, ylab="count")+ggtitle("Survey sentiments")


#Plot two - count of words associated with each sentiment, expressed as a percentage
barplot(
  sort(colSums(prop.table(d[, 1:8]))), 
  horiz = TRUE, 
  cex.names = 0.7, 
  las = 1, 
  main = "Emotions in Text", xlab="Percentage"
)
```

<br/>
Como se observa en las graficas anterior, mas del 25% de los ensayos de los estudiantes logran transmitir la verdad. Mas del 15% logra transmitir anticipacion y mas del 10% logran transmitir felicidad, ademas, se observa que los sentimientos mas bajos son sorpresa, enojo y disgusto. 

```{r}
ineficiente<-subset(db, db$discourse_effectiveness=="Ineffective")
summary(ineficiente)
Efectivo<-subset(db, db$discourse_effectiveness=="Effective")
summary(Efectivo)
Adecuado<-subset(db, db$discourse_effectiveness=="Adequate")
summary(Adecuado)


```

```{r}
resumen<-data.frame(Categoria= c("Ineficiente","Adecuado","Eficiente"),
                    Enojo= c(mean(ineficiente$Enojo),mean(Adecuado$Enojo),mean(Efectivo$Enojo)),
                    Anticipacion= c(mean(ineficiente$Anticipacion),mean(Adecuado$Anticipacion),mean(Efectivo$Anticipacion)),
                    Disgusto= c(mean(ineficiente$Disgusto),mean(Adecuado$Disgusto),mean(Efectivo$Disgusto)),
                    Miedo= c(mean(ineficiente$Miedo),mean(Adecuado$Miedo),mean(Efectivo$Miedo)),
                    Felicidad= c(mean(ineficiente$Felicidad),mean(Adecuado$Felicidad),mean(Efectivo$Felicidad)),
                    Tristeza= c(mean(ineficiente$Tristeza),mean(Adecuado$Tristeza),mean(Efectivo$Tristeza)),
                    Sorpresa= c(mean(ineficiente$Sorpresa),mean(Adecuado$Sorpresa),mean(Efectivo$Sorpresa)),
                    Verdad= c(mean(ineficiente$Verdad),mean(Adecuado$Verdad),mean(Efectivo$Verdad)),
                    Negativo= c(mean(ineficiente$Negativo),mean(Adecuado$Negativo),mean(Efectivo$Negativo)),
                    Positivo= c(mean(ineficiente$Positivo),mean(Adecuado$Positivo),mean(Efectivo$Positivo))
                    
                    )

resumen
```
<br/>
Como se observa los argumentos mas eficientes es cuando poseen mas sentimiento, ya que como se observa anteriormente, la categoria de eficiente lidera en todos los sentimientos, dando a entender que el texto logra expresar de manera eficiente lo que el autor quiere explicar.

##### Cantidad de palabras

```{r}
require(stringr)

db<-read.csv("train.csv")

db$CantidadPalabras<-str_count(db$discourse_text, '\\w+')

ineficiente<-subset(db, db$discourse_effectiveness=="Ineffective")

Efectivo<-subset(db, db$discourse_effectiveness=="Effective")

Adecuado<-subset(db, db$discourse_effectiveness=="Adequate")

resumen<-data.frame(Categoria= c("Ineficiente","Adecuado","Eficiente"),
                    Promedio=c(mean(ineficiente$CantidadPalabras),mean(Efectivo$CantidadPalabras),mean(Adecuado$CantidadPalabras)))

ggplot(data=resumen, aes(x=reorder(Categoria,Promedio), y=Promedio,fill=Categoria)) +
  geom_bar(stat="identity", position=position_dodge())+
  geom_text(aes(label=Promedio), vjust=1.6, color="black",
            position = position_dodge(0.9), size=3.5)+
  labs(title="Promedio de palabras de cada categoria",x='Categoria', y="Promedio")+
  theme(legend.position="none")

```
<br/>
Como se observa en la grafica anterior, que un discurso o texto tenga mas cantidad de palabra, no significa que sera mas comprensible o en este caso mas eficiente. Evidenciando que cantidad no significa calidad.

### Conclusiones y hallazgos

1. Los argumentos con mejor escritura, uso de verbos y sobre todo uso correcto de las palabras, tienden a expresar mas sentimientos en su escritura, facilitando a los lectores comprender al autor.
2. Un argumento demasiado positivo no significa que sera eficiente.
3. Que un discurso tenga demasiadas palabras no signfica que sera eficiente o de mejor comprension.