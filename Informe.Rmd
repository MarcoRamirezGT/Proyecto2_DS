---
title: "Proyecto 2"
author: "Marco Ramirez, Estuardo Hernandez, Alfredo Quezada"
date: "2022-09-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Prediccion de argumentos efectivos

El objetivo de este proyecto es clasificar los elementos argumentativos en la escritura de los estudiantes como "efectivos", "adecuados" o "ineficaces". Esto mediante los datos proporcionados por Kaggle.


### Situacion problematica

### Problema cientifico

### Objetivos

### Descripcion de los datos

### Analisis Exploratorio

```{r Aanalisis Exploratorio}

db<- read.csv('train.csv')


```
La base de datos cuenta con `r nrow(db)` filas y `r ncol(db)` columnas


```{r message=FALSE, warning=FALSE}
# Librerias necesarias

library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("syuzhet")
library("ggplot2")
library(dplyr)
# Limpiamos el texto




```
#### 1. ¿Cuál es el tipo de discurso predominante?

```{r}

disType <- db %>% 
  group_by(db$discourse_type) %>% 
  tally()

colnames(disType)[1]<-'Tipo'
colnames(disType)[2]<-'Cantidad'

ggplot(data=disType, aes(x=reorder(Tipo,Cantidad), y=Cantidad,fill=Tipo)) +
  geom_bar(stat="identity", position=position_dodge())+
  geom_text(aes(label=Cantidad), vjust=1.6, color="black",
            position = position_dodge(0.9), size=3.5)+
  labs(title="Cantidad de cada tipo de discurso",x='Tipo de discurso', y="Cantidad")+
  theme(legend.position="none")
```
<br /> 
Como se observa en la grafica anterior el tipo de discurso predominante es 'Evidence', el cual se refiere a ideas o ejemplos que respaldan afirmaciones, reconvenciones o refutaciones. Y en en segundo puesto se encuentra 'Claim', el cual es un reclamo que respalda alguna posicion. 


#### Cantidad de discursos categorizados como ineficaz, adecuado y eficaz

```{r}
cantType <- db %>% 
  group_by(db$discourse_effectiveness) %>% 
  tally()

colnames(cantType)[1]<-'Tipo'
colnames(cantType)[2]<-'Cantidad'

ggplot(data=cantType, aes(x=reorder(Tipo,Cantidad), y=Cantidad,fill=Tipo)) +
  geom_bar(stat="identity", position=position_dodge())+
  geom_text(aes(label=Cantidad), vjust=1.6, color="black",
            position = position_dodge(0.9), size=3.5)+
  labs(title="Cantidad de cada efectividad de los discursos",x='Efectividad', y="Cantidad")+
  theme(legend.position="none")

```
<br />
Como se observa, los 3 tipos de efectividad no se encuentran balanceados, ademas, la efectividad predominante es 'Adequate' con una cantidad de '20977' discursos. 'Effective' con una cantidad de '9326' e 'Ineffective' con una cantidad de '6462'

#### Analisis del los discursos

##### Analisis del discurso sin limpieza
```{R}

wordcloud(words = db$discourse_text, 
          max.words=100, random.order=FALSE, rot.per=0.40, 
          colors=brewer.pal(8, "Dark2"))



```
< br />
Como se observa en la mayoria de discursos, la palabra con mayor frecuencia es estudiantes, luego people, y electoral, entre ellas.

Ahora veremos como se ve una nube de palabras con una limpieza de datos.

```{r}

TextDoc <- Corpus(VectorSource(db$discourse_text))
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
TextDoc <- tm_map(TextDoc, toSpace, "/")
TextDoc <- tm_map(TextDoc, toSpace, "@")
TextDoc <- tm_map(TextDoc, toSpace, "\\|")
# Convert the text to lower case
TextDoc <- tm_map(TextDoc, content_transformer(tolower))
# Remove numbers
TextDoc <- tm_map(TextDoc, removeNumbers)
# Remove english common stopwords
TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
# Remove your own stop word
# specify your custom stopwords as a character vector
TextDoc <- tm_map(TextDoc, removeWords, c("like", "just", "get",'will','new','now','via','dont','one','can')) 
# Remove punctuations
TextDoc <- tm_map(TextDoc, removePunctuation)
# Eliminate extra white spaces
TextDoc <- tm_map(TextDoc, stripWhitespace)
# Text stemming - which reduces words to their root form
TextDoc <- tm_map(TextDoc, stemDocument)

TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
 # Sort by descearing value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 5 most frequent words
head(dtm_d)
# Plot the most frequent words
barplot(dtm_d[1:5,]$freq, las = 2, names.arg = dtm_d[1:5,]$word,
        col ="lightgreen", main ="Top 5 most frequent words",
        ylab = "Word frequencies")
set.seed(1234)
wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 5,
          max.words=100, random.order=FALSE, rot.per=0.40, 
          colors=brewer.pal(8, "Dark2"))
```

<br /> 
Como se observa la palabra *Student* sigue predominando en los discursos de los estudiantes, asi como *people* y *vote*

##### Analisis de sentimiento en los discursos

```{r}

syuzhet_vector <- get_sentiment(db$discourse_text, method="syuzhet")
# see the first row of the vector
head(syuzhet_vector)
# see summary statistics of the vector
summary(syuzhet_vector)

# bing
bing_vector <- get_sentiment(db$discourse_text, method="bing")
head(bing_vector)
summary(bing_vector)
#affin
afinn_vector <- get_sentiment(db$discourse_text, method="afinn")
head(afinn_vector)
summary(afinn_vector)

#compare the first row of each vector using sign function
rbind(
  sign(head(syuzhet_vector)),
  sign(head(bing_vector)),
  sign(head(afinn_vector))
)

d<-get_nrc_sentiment(db$discourse_text)
# head(d,10) - to see top 10 lines of the get_nrc_sentiment dataframe
head (d,10)


#transpose
td<-data.frame(t(d))
#The function rowSums computes column sums across rows for each level of a grouping variable.
td_new <- data.frame(rowSums(td[2:253]))
#Transformation and cleaning
names(td_new)[1] <- "count"
td_new <- cbind("sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2<-td_new[1:8,]
#Plot One - count of words associated with each sentiment
quickplot(sentiment, data=td_new2, weight=count, geom="bar", fill=sentiment, ylab="count")+ggtitle("Survey sentiments")


#Plot two - count of words associated with each sentiment, expressed as a percentage
barplot(
  sort(colSums(prop.table(d[, 1:8]))), 
  horiz = TRUE, 
  cex.names = 0.7, 
  las = 1, 
  main = "Emotions in Text", xlab="Percentage"
)
```

### Conclusiones y hallazgos

